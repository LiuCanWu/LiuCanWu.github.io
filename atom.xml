<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>劉Yang の Blog</title>
  
  <subtitle>风月同天</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://LiuCanWu.github.io/"/>
  <updated>2020-02-26T09:12:51.149Z</updated>
  <id>https://LiuCanWu.github.io/</id>
  
  <author>
    <name>Liu Yang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>韩韩松_Deep_Compression</title>
    <link href="https://LiuCanWu.github.io/2020/02/25/%E9%9F%A9%E9%9F%A9%E6%9D%BE-Deep-Compression/"/>
    <id>https://LiuCanWu.github.io/2020/02/25/韩韩松-Deep-Compression/</id>
    <published>2020-02-25T07:12:07.000Z</published>
    <updated>2020-02-26T09:12:51.149Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://arxiv.xilesou.top/abs/1510.00149" target="_blank" rel="noopener">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</a></p><h2 id="压缩、量化与编码"><a href="#压缩、量化与编码" class="headerlink" title="压缩、量化与编码"></a>压缩、量化与编码</h2><p>基于上一篇文章的基础上<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener"><strong>Learning both Weights and Connections for Effificient Neural Networks</strong></a>，这篇文章中的”deep compression” 主要有三个步骤，pruning, trianed quantization and Huffman coding.在不影响精度的情况下，这三步能够压缩网络大小35x到49x。</p><p>之前的剪枝能够实现9x到13x的效果，随后的量化将FLP32量化为INT5,实现了进一步的压缩。</p><p>流程图如下：</p><p><img src="1.JPG" alt="1"></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在移动端部署要考虑的问题</p><ul><li>网络规模大小，能不能放进片上SRAM</li><li>功耗，存取数据(占比最大)、数值计算</li></ul><blockquote><p>如45nm CMOS的存储计算设备，一次浮点加法消耗0.9pJ，32bit 的SRAM存取消耗5pJ，32bit DRAM存取消耗640pJ。以一个有1000000个连接的网络为例，以20HZ的频率内存存取，需要的功率为$ (20H_z)(1G)(640pJ) = 12.8W$ </p></blockquote><p>深度神经网络在移动端部署会带来很多好处，如更好的隐私保护，占用更少的网络带宽，以及实时处理。</p><h3 id="Trained-Quantization-and-weight-sharing"><a href="#Trained-Quantization-and-weight-sharing" class="headerlink" title="Trained Quantization and weight sharing"></a>Trained Quantization and weight sharing</h3><p>这一步作者通过参数量化，及共享权重/梯度进一步压缩。通过<strong>聚类</strong>算法相同或相近的值共享同一值来表达，减少所用的存储空间。</p><p><img src="2.JPG" alt="2"></p><p>其中压缩率<strong>r</strong>可用下式表示：</p><p>​                                                            $r = \frac{nb}{nlog_2(k)+kb} $ </p><p>n为连接数，b为原位宽(flp32),k为聚类的类数，$log_2(k)$ 为用来表达该类的索引的位宽。</p><h4 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h4><p>是在网络完全训练之后，通过对每一层的权值进行聚类实现的(在不同层之间没有权值共享)。通过最小化不同类的类间距，公式如下</p><p>​                                                   $arg \quad min \sum_{i=1}^{k} \sum_{\omega{\in}  c_i}^{}|\omega-c_i|^2$</p><h4 id="初始化聚类中心"><a href="#初始化聚类中心" class="headerlink" title="初始化聚类中心"></a>初始化聚类中心</h4><p>实验了三种中心初始化方法，随机、基于概率密度、线性</p><p>由于线性初始化能囊括所有范围的值，在参数中占少数的较大的参数对于模型的整体性能影响至关重要，随机或基于概率的中心初始化对于这些少量的大参数“照顾步骤”， 因此线性初始化效果最好。</p><p><img src="3.JPG" alt="3"></p><h4 id="哈夫曼编码"><a href="#哈夫曼编码" class="headerlink" title="哈夫曼编码"></a>哈夫曼编码</h4><p>常用在无损压缩的最佳预编码，信息熵大的编码位数少，信息熵小的编码位数大。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>4种网络，两类数据集，caffe框架。量化及参数共享是通过维护一个存储这共享的权重的’codebook’结构实现的。哈夫曼编码在所有训练过程完成后进行。</p><p>实验结果如下</p><p><img src="4.JPG" alt="4"></p><p>在AlexNet上的实验结果</p><p><img src="5.JPG" alt="5"></p><h3 id="结果讨论"><a href="#结果讨论" class="headerlink" title="结果讨论"></a>结果讨论</h3><p>和其他方法相比较，pruning+quantization的组合效果更好，比他们单打独斗效果更好。</p><p><img src="6.JPG" alt="6"></p><ul><li>在硬件上的加速及功耗效率</li></ul><p><img src="7.jpg" alt></p><p><img src="8.jpg" alt></p><ul><li>权重、索引(index)及 codebook所占比重</li></ul><p><img src="9.jpg" alt></p><ul><li>与其他论文中的结果比较</li></ul><p><img src="10.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.xilesou.top/abs/1510.00149&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Deep compression: Compressing deep ne
      
    
    </summary>
    
    
      <category term="deep compression" scheme="https://LiuCanWu.github.io/tags/deep-compression/"/>
    
  </entry>
  
  <entry>
    <title>用Typora写Markdown数学公式</title>
    <link href="https://LiuCanWu.github.io/2020/02/23/%E7%94%A8Typora%E5%86%99Markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <id>https://LiuCanWu.github.io/2020/02/23/用Typora写Markdown数学公式/</id>
    <published>2020-02-23T14:46:02.000Z</published>
    <updated>2020-02-24T08:12:10.797Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="在Typoa中选择数学模块"><a href="#在Typoa中选择数学模块" class="headerlink" title="在Typoa中选择数学模块"></a>在Typoa中选择数学模块</h3><ul><li>选择“段落” ——&gt; “公式块”</li><li>快捷键Ctrl+Shift+m</li><li>“$$” + 回车</li></ul><script type="math/tex; mode=display">\frac{-b\pm\sqrt{b^2-4*ac}}{2a}</script><h3 id="添加数学公式"><a href="#添加数学公式" class="headerlink" title="添加数学公式"></a>添加数学公式</h3><ul><li><p>上标：<code>x^2 + y^2 =1</code>  </p><script type="math/tex; mode=display">x^2 + y^2 =1</script></li><li><p>下标：<code>a_1*x_1+a_2 * x_2 = c_1</code> </p><script type="math/tex; mode=display">a_1*x_1+a_2 * x_2 = c_1</script></li><li><p>分式：<code>\frac{-b+8}{3*a}</code>  </p><script type="math/tex; mode=display">\frac{-b+8}{3*a}</script></li><li><p>省略号：<code>\cdots</code> </p><script type="math/tex; mode=display">\cdots</script></li><li><p>均值：<code>\overline{x}</code> </p><script type="math/tex; mode=display">\overline{x}</script></li><li><p>矢量：<code>\vec{a}</code>  </p><script type="math/tex; mode=display">\vec{a}</script></li><li><p>偏导数：<code>\frac{\partial^2 u}{\partial z^2}</code> </p><script type="math/tex; mode=display">\frac{\partial^2 u}{\partial z^2}</script></li><li><p>开根号：<code>\sqrt{b^2-4ac}</code> </p><script type="math/tex; mode=display">\sqrt{b^2-4ac}</script></li><li><p>不定积分：<code>\int{4x^2}dx</code> </p><script type="math/tex; mode=display">\int{4x^2}dx</script></li><li><p>定积分：<code>\int_{1}^{2}{4x^2}dx</code> </p><script type="math/tex; mode=display">\int_{1}^{2}{4x^2}dx</script></li><li><p>极限: <code>\lim_{n\rightarrow+\infty}{\frac{1+n}{\sqrt{n}}}</code> </p><script type="math/tex; mode=display">\lim_{n\rightarrow+\infty}{\frac{1+n}{\sqrt{n}}}</script></li><li><p>公式内换行：<code>a_1+b_1=c_1 \\ a_2+b_2=c_2</code> </p><script type="math/tex; mode=display">a_1+b_1=c_1 \\ a_2+b_2=c_2</script></li><li><p>公式内空格：<code>AAA \quad BBB</code>  </p><script type="math/tex; mode=display">AAA \quad BBB</script></li><li><p>累加：<code>\sum{a} \\ \sum_{i=1}^{100}{a_n}</code> </p><script type="math/tex; mode=display">\sum{a} \\ \sum_{i=1}^{100}{a_n}</script></li><li><p>括号：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\langle  <span class="number">1</span>+<span class="number">1</span>=<span class="number">2</span> \rangle \\</span><br><span class="line">\lceil   <span class="number">1</span>+<span class="number">2</span>=<span class="number">3</span> \rceil \\</span><br><span class="line">\lfloor  <span class="number">1</span>+<span class="number">3</span>=<span class="number">4</span> \rfloor \\</span><br><span class="line">\lbrace  <span class="number">1</span>+<span class="number">4</span>=<span class="number">5</span> \rbrace \\</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\langle  1+1=2 \rangle \\\lceil   1+2=3 \rceil \\\lfloor  1+3=4 \rfloor \\\lbrace  1+4=5 \rbrace \\</script></li></ul><ul><li><p>累乘：<code>\prod{x} \\ \prod_{n=1}^{100}{x_n}</code> </p><script type="math/tex; mode=display">\prod{x} \\ \prod_{n=1}^{100}{x_n}</script></li><li><p>对数：<code>\ln12 - \log_2{32} +lg13 - ln(sqrt{13}+1)</code> </p><script type="math/tex; mode=display">\ln12 - \log_2{32} +lg13 - ln(sqrt{13}+1)</script></li><li><p>运算符1：<code>\frac{-b\pm\sqrt{b^2-4*ac}}{2a}</code>  </p></li></ul><script type="math/tex; mode=display">\frac{-b\pm\sqrt{b^2-4*ac}}{2a}</script><ul><li><p>运算符：<code>\time \\ \cdot \\ \div \\ \neq \\ \equiv \\ \leq \\ \geq</code> </p><script type="math/tex; mode=display">\times \\ \cdot \\ \div \\ \neq \\ \equiv \\ \leq \\ \geq</script></li><li><p>特殊符号: <code>\rightarrow \\ \leftarrow \\ \leftrightarrow \\ \forall \\ \exist \\ \because \\ \therefore</code>  </p><script type="math/tex; mode=display">\rightarrow \\ \leftarrow \\ \leftrightarrow \\ \forall \\ \exist \\ \because \\ \therefore</script></li><li><p>集合符号：<code>\subset \\ \subseteq \\ \in \\ \notin \\ \cup \\ \cap</code> </p><script type="math/tex; mode=display">\subset \\ \subseteq \\ \in \\ \notin \\ \cup \\ \cap</script></li><li><p>其他特殊符号： <code>\infty \\ \emptyset \\ \nabla \\ \bot \\ \angle \\ \mathbb{R} \\ \mathbb{N} \\ \mathbb{Z} \\  \mathbb{S}</code>  </p><script type="math/tex; mode=display">\infty \\ \emptyset \\ \nabla \\ \bot \\ \angle \\ \mathbb{R} \\ \mathbb{N} \\ \mathbb{Z} \\  \mathbb{S}</script></li><li><p>定义一个矩阵:</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A=\left[</span><br><span class="line">\begain&#123;matrix&#125;</span><br><span class="line">1&amp;2&amp;3&amp;\\</span><br><span class="line">2&amp;1&amp;3&amp;\\</span><br><span class="line">3&amp;2&amp;1&amp;</span><br><span class="line">\end&#123;matrix&#125;</span><br><span class="line">\right]</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">A=\left[\begin{matrix}1&2&3&\\2&1&3&\\3&2&1&\end{matrix}\right]</script><ul><li>不同字体</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\rm&#123;A&#125; \\</span><br><span class="line">\cal&#123;A&#125; \\</span><br><span class="line">\it&#123;A&#125; \\</span><br><span class="line">\Bbb&#123;A&#125; \\</span><br><span class="line">\bf&#123;A&#125; \\</span><br><span class="line">\mit&#123;A&#125; \\</span><br><span class="line">\sf&#123;A&#125; \\</span><br><span class="line">\scr&#123;A&#125; \\</span><br><span class="line">\tt&#123;A&#125; \\</span><br><span class="line">\frak&#123;A&#125; \\</span><br><span class="line">\boldsymbol&#123;A&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\rm{A} \\\cal{A} \\\it{A} \\\Bbb{A} \\\bf{A} \\\mit{A} \\\sf{A} \\\scr{A} \\\tt{A} \\\frak{A} \\\boldsymbol{A}</script><ul><li>罗马数字：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\alpha \quad \beta \quad \gamma \quad \delta \\ \epsilon \quad \varepsilon \quad \zeta \quad \eta \\ \theta \quad \iota \quad \kappa \quad \lambda \\ \mu \quad \nu \quad \xi \quad \omicron \\ \pi \quad \rho \quad \sigma \quad \tau \\ \upsilon \quad \varphi \quad \chi \quad \psi \\ \omega</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\alpha \quad \beta \quad \gamma \quad \delta \\ \epsilon \quad \varepsilon \quad \zeta \quad \eta \\ \theta \quad \iota \quad \kappa \quad \lambda \\ \mu \quad \nu \quad \xi \quad \omicron \\ \pi \quad \rho \quad \sigma \quad \tau \\ \upsilon \quad \varphi \quad \chi \quad \psi \\ \omega</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;在Typoa中选择数学模块&quot;&gt;&lt;a href=&quot;#在Typoa中选择数学模块&quot; class=&quot;headerlink&quot; title=&quot;在Typoa中选择数学模块&quot;&gt;&lt;/a&gt;在Typoa中选择数学模块&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;选择
      
    
    </summary>
    
    
      <category term="Markdown" scheme="https://LiuCanWu.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>论文——_Lwc</title>
    <link href="https://LiuCanWu.github.io/2020/02/23/%E8%AE%BA%E6%96%87%E2%80%94%E2%80%94-Lwc/"/>
    <id>https://LiuCanWu.github.io/2020/02/23/论文——-Lwc/</id>
    <published>2020-02-23T06:41:44.000Z</published>
    <updated>2020-02-25T08:02:21.775Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener"><strong>Learning both Weights and Connections for Effificient Neural Networks</strong></a></p><h4 id="Hexo中的LaTex公式渲染问题查一下"><a href="#Hexo中的LaTex公式渲染问题查一下" class="headerlink" title="Hexo中的LaTex公式渲染问题查一下"></a>Hexo中的LaTex公式渲染问题查一下</h4><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>为了传统神经网络运行时所需的存储及计算资源，将层间的神经元的连接权重依据幅度大小排序，在不影响精度的情况下，去除掉那些冗余的不重要的连接。剪枝过程分三步走–&gt; train-prune-retrain. 测试结果，AlexNet在ImafeNet上在没有带来精度损失的条件下参数减少了9倍，VGG-16减少了13倍。</p><table><thead><tr><th>网络模型</th><th>LeNet-5</th><th>AlexNet</th><th>Deepface</th><th>VGG16</th></tr></thead><tbody><tr><td>参数量大小</td><td>&lt;1M</td><td>60M</td><td>120M</td><td>140M</td></tr></tbody></table><ul><li>存储、内存位宽、计算消耗</li></ul><p><img src="1.JPG" alt="energy table"></p><p>从上表可以看出，神经网络在进行运算时主要的能耗用在了数据的读取上，即</p><blockquote><p>SRAM和DRAM， SRAM是片上的静态随机存储，这种存储器只要保持通电，里面存储的数据就可以恒常保持，耗电量小，访问速度快，容量也小；DRAM是片下的动态随机存储器，需要周期性的充电，定时刷新，故耗电量大，访问速度慢，但容量大。</p></blockquote><p>故放不进SRAM的网络模型，只能放进DRAM，同时耗能更多。</p><h3 id="最近进展"><a href="#最近进展" class="headerlink" title="最近进展"></a>最近进展</h3><p>神经网络中冗余的参数，导致了计算与存储的浪费。为了去掉这些冗余之前也有一些工作进行了尝试。如对激活值进行INT8量化(Vanhoucke <em>et al.</em> ),通过近似低秩分解找到神经网络替代的线性结构( Denton <em>et al</em>),或通过向量量化压缩深度卷积网络。这些技术和神经网络剪枝都是正交的，可以组合使用。</p><p>这篇文章中的剪枝过程共三步如下图所示，其中prun-&gt;retrain可以循环执行多轮</p><p><img src="2.JPG" alt="jianzhigc"></p><h3 id="剪枝过程"><a href="#剪枝过程" class="headerlink" title="剪枝过程"></a>剪枝过程</h3><ul><li>最开始训练不是为了得到最终的权重值，而是为了学习那些权重是重要的。然后根据阈值去掉那些不重要的，再retrain恢复精度。循环多次直至符合要求。</li></ul><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>正则方式的选择对剪枝和retrain的表现很有影响，L1正则化得到的剪枝效果不错，但retrain后的表现没有L2正则化好。</p><h4 id="Dropout率调整"><a href="#Dropout率调整" class="headerlink" title="Dropout率调整"></a>Dropout率调整</h4><p>dropout通常是用来防止过拟合的，在retrain阶段dropout比例要随着模型的容量做出相应的调整。随着模型的参数逐渐稀疏，输出结果的预测方差(variance)也会越来越小，防止了过拟合，因此dropout率也要慢慢下调。</p><p>进行定量分析调整的公式如下：</p><p><img src="3.JPG" alt="formula"></p><h4 id="局部剪枝及参数联合适应"><a href="#局部剪枝及参数联合适应" class="headerlink" title="局部剪枝及参数联合适应"></a>局部剪枝及参数联合适应</h4><ol><li>一轮剪枝过后，进行retrain时的初始化权重，保持和原网络的初始化权重一样(针对这一点其实还是要视情况而定，在<a href="https://arxiv.org/abs/1803.03635" target="_blank" rel="noopener">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a>这篇文章中也做了相应的实验)</li><li>为了防止在retrain阶段出现梯度消失等问题，在剪枝完全连接层进行retrain时，会固定住卷积层的参数，反之亦然。</li></ol><h4 id="迭代剪枝"><a href="#迭代剪枝" class="headerlink" title="迭代剪枝"></a>迭代剪枝</h4><p>每轮迭代其实是一个寻找最优连接贪婪搜索的过程，作者也进行了基于绝对值的概率剪枝，但结果不理想。</p><h4 id="剪掉神经元"><a href="#剪掉神经元" class="headerlink" title="剪掉神经元"></a>剪掉神经元</h4><p>在剪枝完一轮后，那些没有输入或输出的神经元可以被安全的剪掉。</p><h3 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h3><p>作者使用Caffe框架在TItanX及GTX980上，对四种网络模型在MNIST及ImageNet数据集上进行了测试。</p><ul><li>AlexNet结果</li></ul><p><img src="4.JPG" alt="Alex"></p><p><img src="5.JPG" alt="Alex"></p><ul><li>VGG16结果</li></ul><p><img src="6.JPG" alt="vgg"></p><p><img src="7.JPG" alt="vgg"></p><ul><li>有趣的发现</li></ul><p>作者发现神经网络剪枝能够发现-visual attention 的区域，如下图所示，是LeNet-300-100这个网络第一个全连接层的激活值的输出，共有28条带，每个带宽28.对应于输入的28x28的像素。由于手写的数字大多集中于图像中间，故这副图的中间区域更亮，两侧被剪枝的幅度更大。</p><p><img src="8.JPG" alt="attention"></p><h3 id="结果讨论"><a href="#结果讨论" class="headerlink" title="结果讨论"></a>结果讨论</h3><p>如下图所示，参数被剪掉的越多，精度损失的越厉害。同时retrain效果显著。</p><p>在有retrain的情况下，L2正则化效果比L1正则化效果更好。L1更适合retrain之前的迫零，L2更适合retrain过程。</p><p><img src="9.JPG" alt="regulation"></p><p>其中剪枝对于网络中权重参数数量，及大小分布的影响如下图所示</p><p><img src="before.PNG" alt></p><p><img src="pruning.PNG" alt></p><p><img src="tuning.png" alt></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>剪枝灵感来源于，哺乳动物大脑的生长机理。同时对全连接层及卷积层进行剪枝，剪枝效果在9x到13x之间。推动了大型神经网络在移动端的部署。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.02626&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Learning both Weights and Connection
      
    
    </summary>
    
    
      <category term="pruning 读论文" scheme="https://LiuCanWu.github.io/tags/pruning-%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow基础</title>
    <link href="https://LiuCanWu.github.io/2020/02/21/TensorFlow%E5%9F%BA%E7%A1%80/"/>
    <id>https://LiuCanWu.github.io/2020/02/21/TensorFlow基础/</id>
    <published>2020-02-21T13:32:52.000Z</published>
    <updated>2020-02-25T14:51:54.702Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><ul><li>TensorFlow支持三种类型的张量<ol><li>常量：为值不可改变的张量(存储在计算图的定义中，是占用内存的)</li><li>变量：当一个值在会话中需要更新时，使用变量来表示。变量在使用前需要被显示初始化。</li><li>占位符：用于将值输入到TensorFlow的图中，可以和feed_dict一起使用来输入数据。在训练神经网络时，通常用于提供新的训练样本。且在运行计算图时，可以为占位符赋值。故在构建一个计算图时不需要真正的输入数据。因此占位符不包含任何数据，不需要初始化它们。</li></ol></li></ul><p>所有常量、变量、占位符将在代码的计算图部分中定义。如果在定义部分使用print语句，输出的不是它的值，而是有关张量类型的信息。</p><p><img src="1.JPG" alt="1"></p><p>为了得到相关的值需要创建会话图，在会话图中进行输出。</p><p><img src="2.JPG" alt="2"></p><ul><li><p>TensorFlow程序的特点为，将程序分为两个独立的部分。1.构建神经网络蓝图，包括计算图的定义。2.在会话中执行计算图。正是图的定义与执行的分开设计让TF能够多平台及并行执行。</p><blockquote><p><strong>会话对象评估张量与操作对象的环境，不同的张量对象的值仅在会话对象中被初始化、访问和保存。在此之前张量对象只被抽象定义，在会话中才被赋予实际的意义。</strong></p></blockquote></li></ul><ul><li>run函数原型：<code>run(fetches,feed_dict=None,options=None,run_metadata)</code> ,其中运算结果在fetches中提取，为了得到fetches的结果所需的变量由<code>feed_dict</code> 提供。</li></ul><h3 id="如何指定CPU或GPU操作"><a href="#如何指定CPU或GPU操作" class="headerlink" title="如何指定CPU或GPU操作"></a>如何指定CPU或GPU操作</h3><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>在 TensorFlow 中可以通过三种方式读取数据：</p><ol><li>通过feed_dict传递数据；</li><li>从文件中读取数据；</li><li>使用预加载的数据；</li></ol><ul><li>在进行多元线性回归时，由于每个特征具有不同的值范围，归一化变得至关重要。</li><li>还可以使用 tf.summary.histogram 可视化梯度、权重或<strong>特定层的输出分布</strong>：</li></ul><blockquote><p><strong>激活函数的作用</strong></p><p>每个神经元都必须有激活函数。它们为神经元提供了模拟复杂非线性数据集所必需的非线性特性。该函数取所有输入的加权和，进而生成一个输出信号。你可以把它看作输入和输出之间的转换。使用适当的激活函数，可以将输出值限定在一个定义的范围内。</p></blockquote><h3 id="tf-一系列方法学习"><a href="#tf-一系列方法学习" class="headerlink" title="tf.(一系列方法学习)"></a>tf.(一系列方法学习)</h3><ul><li>tf.argmax(input, axis):根据axis的不同返回每行或每列最大值的索引。</li><li>tf.cast(x, dtype, name=None): 进行张量数据类型转换</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow支持三种类型的张量&lt;ol&gt;
&lt;li&gt;常量：为值不可改变的张量(存储在计算图的定义中，是占用内存的)&lt;/li&gt;
&lt;li&gt;变量：当一个值在会话中需要更新时，使用变量来表示。变量在使用前需要被显示初始化。&lt;/li
      
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://LiuCanWu.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Quantization</title>
    <link href="https://LiuCanWu.github.io/2020/02/21/Quantization%E6%A6%82%E8%BF%B0/"/>
    <id>https://LiuCanWu.github.io/2020/02/21/Quantization概述/</id>
    <published>2020-02-21T13:05:06.000Z</published>
    <updated>2020-02-22T09:04:37.642Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>量化是指减少表示一个数字时所用的位数，在研究或部署深度学习算法时主要的表示形式为32位的浮点数表示(FP32)。为了减少带宽与计算量，学界尝试使用更低的精度表示。并且已经证实在没有造成精度显著下降的条件下，权重及激活值可以使用8位整型表示INT8。当然，用更少的位数表示如4/2/1-bits表示，也在积极探索中。</p><h2 id="练一下插入数学公式，文章底部引用"><a href="#练一下插入数学公式，文章底部引用" class="headerlink" title="练一下插入数学公式，文章底部引用"></a>练一下插入数学公式，文章底部引用</h2><h2 id="动机：overall-efficiency"><a href="#动机：overall-efficiency" class="headerlink" title="动机：overall efficiency"></a>动机：overall efficiency</h2><p>进行量化的显著优势是，能明显降低带宽与所用存储。如用INT8来表示权重与激活值相较与FP32表示能减少4倍带宽利用。此外，整形运算比浮点计算更快，存储空间及耗能更少。</p><div class="table-container"><table><thead><tr><th>INT8 Operation</th><th>Energy Saving vs FP32</th><th>Area Saving vs FP32</th></tr></thead><tbody><tr><td>Add</td><td>30x</td><td>116x</td></tr><tr><td>Multiply</td><td>18.5x</td><td>27x</td></tr></tbody></table></div><p>若用更少的位数量化，如二进制(-1,1)或三进制(-1,0,1).则卷积层与全连接层只需加减操作，完全不需要乘法。若激活值也用二进制表示，那加法也不需要了，只需要进行位操作<a href="https://nervanasystems.github.io/distiller/quantization.html#rastegari-et-al-2016" target="_blank" rel="noopener">Rastegari et al., 2016</a></p><h2 id="Integer-vs-FP32"><a href="#Integer-vs-FP32" class="headerlink" title="Integer vs. FP32"></a>Integer vs. FP32</h2><p>数字格式主要有两个属性，1. 动态范围，指可表示的数的范围。2. 在动态范围内能够表示多少数值，决定了精度/分辨率。</p><p>对于整形格式它的动态范围是<img src="1.JPG" alt="range">, n是位数，如INT8的范围是[-128, 127].对于FP32它的动态范围大约是<img src="2.JPG" alt="fprange">.可以看出FP32能够表示更多的值，这对于深度学习模型来说很好。位了解决动态范围窄的问题，通常会加入scale factor将模型中向量的动态范围映射到整形范围中。但精度低的问题依然没有解决。</p><p>需要指出，大多数情况下scale factor是一个浮点数，即使使用整数表示，也避免不了浮点运算。</p><h2 id="避免溢出"><a href="#避免溢出" class="headerlink" title="避免溢出"></a>避免溢出</h2><p>卷积及全连接层中包括在加法器存储中间值，限于整形格式有限的动态范围，对于加法器如果使用和权重及激活值相同的位宽，很肯能会溢出很快，因此，加法器通常使用更高的位宽。</p><h2 id="INT8量化"><a href="#INT8量化" class="headerlink" title="INT8量化"></a>INT8量化</h2><p>大多数情况，可以对一个FP32的模型直接进行INT8量化，在没有re-training的情况下，可得到精度损失很小的结果，若进行fine-tunning可进一步提高精度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;量化是指减少表示一个数字时所用的位数，在研究或部署深度学习算法时主要的表示形式为32位的浮点数表示(FP32)。为了减少带宽与计算量，学界尝试使用更低的精度表示。并且已经证实在没有造成精度显著下降的条件下，权重及激活值可以使用8位整型表示
      
    
    </summary>
    
    
      <category term="model compress quantization" scheme="https://LiuCanWu.github.io/tags/model-compress-quantization/"/>
    
  </entry>
  
  <entry>
    <title>Regularization概述</title>
    <link href="https://LiuCanWu.github.io/2020/02/21/Regularization%E6%A6%82%E8%BF%B0/"/>
    <id>https://LiuCanWu.github.io/2020/02/21/Regularization概述/</id>
    <published>2020-02-21T08:19:03.000Z</published>
    <updated>2020-02-21T13:27:59.862Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="正则化-regularization"><a href="#正则化-regularization" class="headerlink" title="正则化(regularization)"></a>正则化(regularization)</h2><p>在 <a href="https://nervanasystems.github.io/distiller/regularization.html#deep-learning" target="_blank" rel="noopener">Deep Learning</a>这本书中，Goodfellow等人提到正则化 为：”为了降低泛化误差，而非训练误差，对学习算法的改动“</p><p>PyTorch的优化器中使用L2参数正则化来限制模型的容量(减小方差)</p><p>通常可写为：</p><p><img src="1.JPG" alt="1"></p><p>更特殊则写为：</p><p><img src="2.JPG" alt="2"></p><p>其中W是网络中权重参量的集合，<img src="3.JPG" alt="3">是总的训练损失，<img src="4.JPG" alt="4">是数据loss(如目标函数)。</p><p>λR是个标量，是正则的力度。用来平衡data error 与 regularization error,在PyTorch中就是参数<code>weighr_decay</code> </p><p>此外，L2正则化表达式为</p><p><img src="5.JPG" alt="5"></p><h2 id="稀疏化与正则化的关系"><a href="#稀疏化与正则化的关系" class="headerlink" title="稀疏化与正则化的关系"></a>稀疏化与正则化的关系</h2><p>正则化与一些DNN引入稀疏的方法有关。在<a href="https://arxiv.org/abs/1607.04381" target="_blank" rel="noopener">DSD</a>中，韩松等人用剪枝作为正则化方式来提升模型的精度。并且解释到：”稀疏是正则化的一种有力方式，我们的直觉，在给定稀疏度限制的条件下，一旦网络达到了一个局部最小值，降低限制给网路更多的自由来逃离鞍点，到另一个精度更高的局部最小“</p><p>当然，正则化也可用来引入稀疏度，如L 1-norm</p><p><img src="6.jpg" alt="l1-norm"></p><p>L2正则化也可以通过减小大的参数，避免过拟合提升模型精度。但没有迫使这些参数为零。L1正则化使得一些参数为零，因此限制了模型的容量，同时简化了模型。这可以看作是一种特征选择(feature selection)</p><p>在distiller库中，可以设置<code>weight_deacy</code> 参数是否为零，来搭配使用两种正则化方式</p><p><img src="7.jpg" alt></p><p>并且在类<code>distiller.L1Regularization</code> 中实现了L1正则化。</p><h2 id="组正则化-Group-Regularization"><a href="#组正则化-Group-Regularization" class="headerlink" title="组正则化(Group Regularization)"></a>组正则化(Group Regularization)</h2><p>在组正则化中，我们将惩罚整组的参数，而不是单个的。这个组结构是预先定义的。</p><p>对于数据损失，以及基于元素的正则化，可以加入组正则化(Group regularization),将某一层的组参数表示为<img src="8.JPG" alt="Wl">,将其作为惩罚的一部分加入，损失的整体表示为：</p><p><img src="9.JPG" alt="loss"></p><p>组正则化也可称为块正则化，结构正则化，或粗粒度稀疏。组稀疏化是规整的，因此有利于提高推理速度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;正则化-regularization&quot;&gt;&lt;a href=&quot;#正则化-regularization&quot; class=&quot;headerlink&quot; title=&quot;正则化(regularization)&quot;&gt;&lt;/a&gt;正则化(regulariz
      
    
    </summary>
    
    
      <category term="model pruning" scheme="https://LiuCanWu.github.io/tags/model-pruning/"/>
    
  </entry>
  
  <entry>
    <title>Pruning概述</title>
    <link href="https://LiuCanWu.github.io/2020/02/20/Pruning%E6%A6%82%E8%BF%B0/"/>
    <id>https://LiuCanWu.github.io/2020/02/20/Pruning概述/</id>
    <published>2020-02-20T14:38:15.000Z</published>
    <updated>2020-02-21T08:08:45.859Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>对神经网络的权重及激活值进行稀疏化的常见方法就是剪枝。剪枝过程中满足剪枝标准的权重会被赋值为零，确保这些被“剪掉” 的元素不会参与反向传播过程。</p><p>神经网络中的权重，偏置，激活值都可以视为被剪的对象。然而偏置的数量少且对每一层的输出影响较大，故没必要剪。</p><h2 id="稀疏度-sparsity"><a href="#稀疏度-sparsity" class="headerlink" title="稀疏度(sparsity)"></a>稀疏度(sparsity)</h2><p>稀疏度用来度量向量中零有多少，若向量中大多数元素都是零，可视为稀疏的。L0正则函数常被用来测量一向量中零元素的多少,既</p><p><img src="1.png" alt="1"></p><p>当且仅当某一项为零时它对整体的贡献才为零。</p><p>在distiller这个库中可以使用<code>distiller.sparity</code> 来获取pytorch向量的稀疏度。</p><h2 id="什么是权重剪枝-weights-pruning"><a href="#什么是权重剪枝-weights-pruning" class="headerlink" title="什么是权重剪枝(weights pruning)"></a>什么是权重剪枝(weights pruning)</h2><p>权重或模型剪枝是网络中权重稀疏度的一种方法，通常情况下parameters指的是权重及偏置向量，因为相较于权重，模型中的偏置变量很少，不值得考虑。</p><p>剪枝需要选择一个标准来决定‘剪掉’谁—剪枝标准。最常见的剪枝标准就是绝对值。将某一权重变量的绝对值与选定的阈值比较，若小于阈值则被值为零(被剪掉),在distiller库中可以使用<code>distiller.MagnitudeParameterPruner</code> 类。根据就是，绝对值小的权重对于最终的输出结果贡献不大，故不太重要可以被减去。</p><p>模型可以被剪枝的另外一个原因是，模型通常是过参的，有很多冗余的参数及特征，因此一部分冗余可以通过将权重设为零被移除。</p><p>考虑模型剪枝的另外一个角度是，寻找一组包含尽可能多的零的权重，并且相较于原模型的精度仍可接受。可以想象参数空间的维度非常高，在这个很高维的解周围可能存在某些稀疏解，我们就是想找出这些稀疏解。</p><h2 id="剪枝步骤-schedule"><a href="#剪枝步骤-schedule" class="headerlink" title="剪枝步骤(schedule)"></a>剪枝步骤(schedule)</h2><p>最直接的方式就是对训练过的模型剪一次，也称’one-shot pruning’ 在[ <a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">Learning both Weights and Connections for Efficient Neural Networks</a>这篇文章中，韩松等人提到”free lunch“的效果，—在没有retrain和损失精度的条件下，可以将connections降至原来的1/2.</p><p>同时指出，在prunning之后再retrain可以得到更好的结果，也就是循环剪枝(iterative pruning),retrain的步骤称为fine-tuning。每回合的剪枝标准，回合数，剪枝频率，剪掉谁—&gt;统称为剪枝规划(pruning schedule)。循环剪枝也就是根据criteria去除掉那些不重要的元素，然后通过retrain调整剩余的权重，将模型精度恢复到原模型的水平。</p><p>何时停止剪枝，也根据具体的剪枝算法写在了schedule中。比如要实现某一目标稀疏度，当达到此稀疏度将会停止剪枝。或这根据所需的计算量等其他条件。</p><h2 id="剪枝粒度-granularity"><a href="#剪枝粒度-granularity" class="headerlink" title="剪枝粒度(granularity)"></a>剪枝粒度(granularity)</h2><p>将单个权重作为剪枝单位称为element-wise pruning， 或fine-grained pruning。</p><p>粗粒度的剪枝通常为结构性剪枝，如通道剪枝(channel), 核(filter).</p><h2 id="敏感度分析-sensitivity"><a href="#敏感度分析-sensitivity" class="headerlink" title="敏感度分析(sensitivity)"></a>敏感度分析(sensitivity)</h2><p>通过剪枝引入稀疏度的一大难点是，在每一层中怎样决定阈值(threshold)或稀疏度。敏感度分析是一种根据向量的敏感度进行排序再剪枝的方法。</p><p>具体是在特定层设定剪枝比例，进行一次剪枝操作后，在测试数据集上进行一次精度评估。在所有的含参层都进行同样的操作，每一层实验多个不同的剪枝比例。找出适合于每层的剪枝比例。</p><p>我们想要模型在达到最高进度水平后，对某一层进行剪枝操作对整体性能的影响。</p><p>当然在进行结构性剪枝时，我们也能对整体结构进行敏感度分析。</p><p>在韩松等人的那篇论文中同样进行了element-wise 敏感度分析，结果如下图</p><p><img src="2.PNG" alt></p><p>在AlexNet中卷积层对于剪枝更敏感，并且随着层次加深敏感度降低。同时，全连接层更加不敏感，很好，因为网络中的大多数参数都存在于全连接层之中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;剪枝&quot;&gt;&lt;a href=&quot;#剪枝&quot; class=&quot;headerlink&quot; title=&quot;剪枝&quot;&gt;&lt;/a&gt;剪枝&lt;/h2&gt;&lt;p&gt;对神经网络的权重及激活值进行稀疏化的常见方法就是剪枝。剪枝过程中满足剪枝标准的权重会被赋值为零，确保这些
      
    
    </summary>
    
    
      <category term="model pruning" scheme="https://LiuCanWu.github.io/tags/model-pruning/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://LiuCanWu.github.io/2020/02/15/Pruning_%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D/"/>
    <id>https://LiuCanWu.github.io/2020/02/15/Pruning_网络剪枝/</id>
    <published>2020-02-15T07:12:54.383Z</published>
    <updated>2020-02-15T14:39:24.827Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-能够进行剪枝的原因"><a href="#1-能够进行剪枝的原因" class="headerlink" title="1.能够进行剪枝的原因"></a>1.能够进行剪枝的原因</h3><a id="more"></a><h3 id="2-进行剪枝得到依据"><a href="#2-进行剪枝得到依据" class="headerlink" title="2. 进行剪枝得到依据"></a>2. 进行剪枝得到依据</h3><h3 id="3-很重要的一点"><a href="#3-很重要的一点" class="headerlink" title="3.很重要的一点"></a>3.很重要的一点</h3><blockquote><ol><li><p>不论是再网络训练期间（如vgg16），还是这种剪枝压缩，或用MobileNet的卷及方式代替传统的卷积方法，一层一层的训练或替换都很关键</p></li><li><p>进行非结构性剪枝产生的权重矩阵是稀疏的，再没有定制的硬件及软件库支持的情况下，达不到压缩与加速的效果。</p></li><li></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-能够进行剪枝的原因&quot;&gt;&lt;a href=&quot;#1-能够进行剪枝的原因&quot; class=&quot;headerlink&quot; title=&quot;1.能够进行剪枝的原因&quot;&gt;&lt;/a&gt;1.能够进行剪枝的原因&lt;/h3&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>用Typora写Hexo博客</title>
    <link href="https://LiuCanWu.github.io/2020/02/13/%E7%94%A8Typora%E5%86%99Hexo%E5%8D%9A%E5%AE%A2/"/>
    <id>https://LiuCanWu.github.io/2020/02/13/用Typora写Hexo博客/</id>
    <published>2020-02-13T06:56:08.000Z</published>
    <updated>2020-02-21T08:19:16.545Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Markdown 的一些基本语法</strong> </p><a id="more"></a><p><em>斜体</em></p><p><strong>粗体</strong>   <strong>粗体</strong></p><p><strong><em>分割线</em></strong>   ——&gt; 三个星号</p><hr><hr><p><strong>删除线</strong> —-&gt; 文字两端加个~</p><p><del>删除</del></p><p><strong>下划线 —&gt;</strong>  两端加 <u> </u></p><p><u>带下划线文本</u></p><p><strong>注脚</strong></p><p><sup><a href="#fn_ 注脚内容 " id="reffn_ 注脚内容 "> 注脚内容 </a></sup></p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><ul><li>一： 无序列表使用（*、+、或 -）</li><li><ul><li>二级序列</li></ul></li><li><ul><li><ul><li>三级序列</li></ul></li></ul></li></ul><h2 id="区块"><a href="#区块" class="headerlink" title="区块"></a>区块</h2><ul><li>使用 “ &gt; ”</li></ul><blockquote><p>使用区块进行显示</p><blockquote><p>可以进行嵌套</p><blockquote><p>这是第三级</p></blockquote></blockquote><p>在区块中使用列表</p><blockquote><ol><li>第一项</li><li>第二项</li></ol><ul><li>第一项</li><li>第二项</li></ul></blockquote></blockquote><ul><li><p>在列表中使用区块</p><blockquote><p>列表内区块</p><p>这些使用是正交的</p></blockquote></li></ul><h2 id="代码格式"><a href="#代码格式" class="headerlink" title="代码格式"></a>代码格式</h2><ul><li><p>单个函数使用(<code>` </code> )  例如:    <code>print()</code> 函数</p></li><li><p><strong>代码区块:</strong>    使用四个空格, 或者一个Tab键 ,或使用` <figure class="highlight plain"><figcaption><span>对代码段包裹(可以指定语言格式)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​</span><br><span class="line"></span><br><span class="line">```python </span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python import keras</span><br><span class="line">from tensorflow_model_optimization.python.core.clustering.keras import cluster</span><br></pre></td></tr></table></figure></p></li></ul><h2 id="链接格式"><a href="#链接格式" class="headerlink" title="链接格式"></a>链接格式</h2><ul><li><code>[连接注释](连接地址)</code></li><li>百度网址连接: <a href="https://www.baidu.com" target="_blank" rel="noopener">百度搜索</a></li><li>我的博客主页: <a href="https://liucanwu.github.io">劉YangのBlog</a></li><li>论文连接: <a href="https://arxiv.xilesou.top/pdf/1611.06440.pdf" target="_blank" rel="noopener">Pruning Convolutional Neural Networks for Resource Efficient Inference</a></li></ul><blockquote><p>高级连接</p></blockquote><ul><li>谷歌搜索: <a href="http://www.google.com" target="_blank" rel="noopener">Google</a></li></ul><h2 id="图片格式"><a href="#图片格式" class="headerlink" title="图片格式"></a>图片格式</h2><p><code>![alt 属性文本](图片地址 &quot;可选标题&quot;)</code></p><p><img src="2020all-star.jpg" alt="2020allstar"></p><ul><li>当然图片网址可以像链接地址,中的高级链接那样</li><li>目前Markdown还无法指定图片的高度与宽度, 可以使用普通的<img> 标签</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src=<span class="string">"http://static.runoob.com/images/runoob-logo.png"</span> width=<span class="string">"50%"</span>&gt;</span><br></pre></td></tr></table></figure><p><img src="http://static.runoob.com/images/runoob-logo.png" width="50%"></p><h2 id="Markdown-表格"><a href="#Markdown-表格" class="headerlink" title="Markdown 表格"></a>Markdown 表格</h2><blockquote><p>使用<code>|</code> 来分割不同的单元格, 使用<code>-</code>  来分割表头与其他行</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:left">1</th><th>2</th></tr></thead><tbody><tr><td style="text-align:left"></td></tr></tbody></table></div><h2 id="高级技巧"><a href="#高级技巧" class="headerlink" title="高级技巧"></a>高级技巧</h2><ul><li>LaTex 公式, 需要用 $$ 对其进行包裹</li></ul><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 =  \begin&#123;vmatrix&#125; </span><br><span class="line">\mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; <span class="number">0</span> \\</span><br><span class="line">\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp;  \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; <span class="number">0</span> \\</span><br><span class="line">\end&#123;vmatrix&#125;</span><br><span class="line">$&#123;$tep1&#125;&#123;\style&#123;visibility:hidden&#125;&#123;(x+<span class="number">1</span>)(x+<span class="number">1</span>)&#125;&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\\frac{\partial X}{\partial u} &  \frac{\partial Y}{\partial u} & 0 \\\frac{\partial X}{\partial v} &  \frac{\partial Y}{\partial v} & 0 \\\end{vmatrix}${$tep1}{\style{visibility:hidden}{(x+1)(x+1)}}</script><h2 id="绘制流程图-时序图-甘特图"><a href="#绘制流程图-时序图-甘特图" class="headerlink" title="绘制流程图, 时序图, 甘特图"></a>绘制流程图, 时序图, 甘特图</h2><ul><li><p><strong>1 横向流程图</strong>  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[方形] --&gt;B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt;|a=1| D[结果1]</span><br><span class="line">    C --&gt;|a=2| E[结果2]</span><br><span class="line">    F[横向流程图]</span><br></pre></td></tr></table></figure></li><li><p><strong>2 竖向流程图</strong></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">graph TD</span><br><span class="line">A[方形] --&gt; B(圆角)</span><br><span class="line">    B --&gt; C&#123;条件a&#125;</span><br><span class="line">    C --&gt; |a=1| D[结果1]</span><br><span class="line">    C --&gt; |a=2| E[结果2]</span><br><span class="line">    F[竖向流程图]</span><br></pre></td></tr></table></figure><p><strong>3. 甘特图</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">    gantt</span><br><span class="line">    dateFormat  YYYY-MM-DD</span><br><span class="line">    title 软件开发甘特图</span><br><span class="line">    section 设计</span><br><span class="line">    需求                      :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">    原型                      :active,  des2, 2014-01-09, 3d</span><br><span class="line">    UI设计                     :         des3, after des2, 5d</span><br><span class="line">未来任务                     :         des4, after des3, 5d</span><br><span class="line">    section 开发</span><br><span class="line">    学习准备理解需求                      :crit, done, 2014-01-06,24h</span><br><span class="line">    设计框架                             :crit, done, after des2, 2d</span><br><span class="line">    开发                                 :crit, active, 3d</span><br><span class="line">    未来任务                              :crit, 5d</span><br><span class="line">    耍                                   :2d</span><br><span class="line">    section 测试</span><br><span class="line">    功能测试                              :active, a1, after des3, 3d</span><br><span class="line">    压力测试                               :after a1  , 20h</span><br><span class="line">    测试报告                               : 48h</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    gantt</span><br><span class="line">    dateFormat  YYYY-MM-DD</span><br><span class="line">    title 软件开发甘特图</span><br><span class="line">    section 设计</span><br><span class="line">    需求                      :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">    原型                      :active,  des2, 2014-01-09, 3d</span><br><span class="line">    UI设计                     :         des3, after des2, 5d</span><br><span class="line">未来任务                     :         des4, after des3, 5d</span><br><span class="line">    section 开发</span><br><span class="line">    学习准备理解需求                      :crit, done, 2014-01-06,24h</span><br><span class="line">    设计框架                             :crit, done, after des2, 2d</span><br><span class="line">    开发                                 :crit, active, 3d</span><br><span class="line">    未来任务                              :crit, 5d</span><br><span class="line">    耍                                   :2d</span><br><span class="line">    section 测试</span><br><span class="line">    功能测试                              :active, a1, after des3, 3d</span><br><span class="line">    压力测试                               :after a1  , 20h</span><br><span class="line">    测试报告                               : 48h</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Markdown 的一些基本语法&lt;/strong&gt; &lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>artical</title>
    <link href="https://LiuCanWu.github.io/2019/08/03/artical/"/>
    <id>https://LiuCanWu.github.io/2019/08/03/artical/</id>
    <published>2019-08-03T06:33:05.000Z</published>
    <updated>2020-02-13T06:50:55.497Z</updated>
    
    <content type="html"><![CDATA[<h4 id="摘录自"><a href="#摘录自" class="headerlink" title="摘录自"></a>摘录自</h4><p><a href="http://c.biancheng.net/cplus/" target="_blank" rel="noopener">http://c.biancheng.net/cplus/</a></p><h1 id="一"><a href="#一" class="headerlink" title="一"></a>一</h1><blockquote><p><strong>类只是一张图纸，起到说明的作用，不占用内存空间；对象才是具体的零件，要有地方来存放，才会占用内存空间。</strong></p><p><strong>类只是一个模板（Template），编译后不占用内存空间，所以在定义类时不能对成员变量进行初始化，因为没有地方存储数据。只有在创建对象以后才会给成员变量分配内存，这个时候就可以赋值了</strong></p></blockquote><a id="more"></a><blockquote><p><strong>创建的对象 stu 在栈上分配内存，需要使用<code>&amp;</code>获取它的地址,</strong></p><p><strong>当然，也可以在堆上创建对象，这个时候就需要使用前面讲到的<code>new</code>关键字.也就是说，使用 new 在堆上创建出来的对象是匿名的，没法直接使用，必须要用一个指针指向它，再借助指针来访问它的成员变量或成员函数。</strong></p><p>有了对象指针后，可以通过箭头<code>-&gt;</code>来访问对象的成员变量和成员函数,通过对象名字访问成员使用点号<code>.</code>，通过对象指针访问成员使用箭头<code>-&gt;</code>，这和结构体非常类似。</p><p>  在 C++ 中，通过类名就可以创建对象，即将图纸生产成零件，这个过程叫做类的实例化，因此也称对象是类的一个实例（Ins<a href="http://c.biancheng.net/ref/tan.html" target="_blank" rel="noopener">tan</a>ce）。</p><p>有些资料也将类的成员变量称为属性（Property），将类的成员函数称为方法（Method）。  </p><p>C++ 中的输入与输出可以看做是一连串的数据流，输入即可视为从文件或键盘中输入程序中的一串数据流，而输出则可以视为从程序中输出一连串的数据流到显示屏或文件中。</p><p>cout 和 cin 都是 C++ 的内置对象，而不是关键字。C++ 库定义了大量的类（Class），程序员可以使用它们来创建对象，cout 和 cin 就分别是 ostream 和 istream 类的对象，只不过它们是由标准库的开发者提前创建好的，可以直接拿来使用。这种在 C++ 中提前创建好的对象称为内置对象。</p><p>C语言并没有彻底从语法上支持“真”和“假”，只是用 0 和非 0 来代表。这点在 <a href="http://c.biancheng.net/cplus/" target="_blank" rel="noopener">C++</a> 中得到了改善，C++ 新增了 <strong>bool 类型（布尔类型）</strong>，它一般占用 1 个字节长度。bool 类型只有两个取值，true 和 false：true 表示“真”，false 表示“假</p><p>  但在<a href="http://c.biancheng.net/cplus/" target="_blank" rel="noopener">C++</a>中，这完全没有必要。C++ 允许多个函数拥有相同的名字，只要它们的参数列表不同就可以，这就是函数的重载（Function Overloading）。借助重载，一个函数名可以有多种用途。参数列表又叫参数签名，包括参数的类型、参数的个数和参数的顺序，只要有一个不同就叫做参数列表不同。 </p><p>从这个角度讲，函数重载仅仅是语法层面的，本质上它们还是不同的函数，占用不同的内存，入口地址也不一样 </p><p><code>::</code>被称为域解析符（也称作用域运算符或作用域限定符），用来连接类名和函数名，指明当前函数属于哪个类。</p></blockquote><h2 id="二"><a href="#二" class="headerlink" title="二"></a>二</h2><blockquote><p>在类的内部（定义类的代码内部），无论成员被声明为 public、protected 还是 private，都是可以互相访问的，没有访问权限的限制。<strong>并且类的声明和成员函数的定义都是类定义的一部分</strong></p><p>在类的外部（定义类的代码之外），只能通过对象访问成员，并且通过对象只能访问 public 属性的成员，不能访问 private、protected 属性的成员。  </p><p>除了 set 函数和 get 函数，在创建对象时还可以调用构造函数来初始化各个成员变量，我们将在《<a href="http://c.biancheng.net/view/2221.html" target="_blank" rel="noopener">C++构造函数</a>》一节中展开讨论。不过构造函数只能给成员变量赋值一次，以后再修改还得借助 set 函数</p><p>在栈上创建对象时，实参位于对象名后面，例如<code>Student stu(&quot;小明&quot;, 15, 92.5f)</code>；在堆上创建对象时，实参位于类名后面，例如<code>new Student(&quot;李华&quot;, 16, 96)</code>。</p><p>构造函数没有返回值，因为没有变量来接收返回值，即使有也毫无用处，这意味着：不管是声明还是定义，函数名前面都不能出现返回值类型，即使是 void 也不允许；函数体中不能有 return 语句。</p></blockquote><h2 id="四-：继承与派生"><a href="#四-：继承与派生" class="headerlink" title="四 ：继承与派生"></a>四 ：继承与派生</h2><ol><li><p>派生类除了拥有基类的成员，还可以定义自己的新成员，以增强类的功能。</p></li><li><p>如果希望基类的成员既<strong>不向外暴露（不能通过对象访问</strong>），还能在派生类中使用，那么只能声明为 protected。</p></li><li><p>| 继承方式/基类成员 | public成员 | protected成员 | private成员 |<br>| ————————- | ————— | ——————- | —————- |<br>| public继承        | public     | protected     | 不可见      |<br>| protected继承     | protected  | protected     | 不可见      |<br>| private继承       | private    | private       | 不可见      |</p></li><li><p>因为 m_hobby 是 private 属性的，在派生类中不可见，所以只能借助基类的 public 成员函数 sethobby()、gethobby() 来访问。在派生类中访问基类 private 成员的唯一方法就是借助基类的非 private 成员函数，如果基类没有非 private 成员函数，那么该成员在派生类中将无法访问。</p></li><li><p>如果派生类中的成员（包括成员变量和成员函数）和基类中的成员重名，那么就会遮蔽从基类继承过来的成员，即:<strong>基类成员函数和派生类成员函数不会构成重载</strong>，如果派生类有同名函数，那么就会遮蔽基类中的所有同名函数，不管它们的参数是否一样.</p></li><li><p>类的构造函数不能被继承。构造函数不能被继承是有道理的，因为即使继承了，它的名字和派生类的名字也不一样，不能成为派生类的构造函数，当然更不能成为普通的成员函数。</p></li><li><p>在派生类的构造函数中调用基类的构造函数，来对基类的private 或 protected 成员进行初始化。<strong>因为基类构造函数不会被继承</strong>，不能当做普通的成员函数来调用。换句话说，只能将基类构造函数的调用放在函数头部，不能放在函数体中</p></li><li><p><strong>虚继承（Virtual Inheritance）</strong></p><p>为了解决多继承时的命名冲突和冗余数据问题，<a href="http://c.biancheng.net/cplus/" target="_blank" rel="noopener">C++</a> 提出了虚继承，使得在派生类中只保留一份间接基类的成员。虚继承的目的是让某个类做出声明，承诺愿意共享它的基类。其中，这个被共享的基类就称为虚基类（Virtual Base Class）。在这种机制下，不论虚基类在继承体系中出现了多少次，在派生类中都只包含一份虚基类的成员。</p></li><li><p><strong>向上转型（Upcasting)</strong></p><p>类其实也是一种数据类型，也可以发生数据类型转换，不过这种转换只有在基类和派生类之间才有意义，向上转型是只能将派生类赋值给基类</p><p>将派生类对象赋值给基类对象时，会舍弃派生类新增的成员，也就是“大材小用”</p><ol><li><strong>将派生类指针赋值给基类指针</strong></li></ol><p>编译器通过指针来访问成员变量，指针指向哪个对象就使用哪个对象的数据；编译器通过指针的类型来访问成员函数，指针属于哪个类的类型就使用哪个类的函数。</p><ol><li><strong>因为引用和指针并没有本质上的区别，引用仅仅是对指针进行了简单封装</strong></li></ol><h2 id="五：多态与虚函数"><a href="#五：多态与虚函数" class="headerlink" title="五：多态与虚函数"></a>五：多态与虚函数</h2><p>1.换句话说，<strong>通过基类指针只能访问派生类的成员变量，但是不能访问派生类的成员函数</strong>。为了消除这种尴尬，让基类指针能够访问派生类的成员函数，<a href="http://c.biancheng.net/cplus/" target="_blank" rel="noopener">C++</a> 增加了<strong>虚函数（Virtual Function）</strong>。使用虚函数非常简单，只需要在函数声明前面增加 virtual 关键字。  </p><p>有了虚函数，基类指针指向基类对象时就使用基类的成员（包括成员函数和成员变量），指向派生类对象时就使用派生类的成员。换句话说，基类指针可以按照基类的方式来做事，也可以按照派生类的方式来做事，它有多种形态，或者说有多种表现方式，我们将这种现象称为<strong>多态（Polymorphism）</strong></p><p><strong>多态是指通过基类的指针既可以访问基类的成员，也可以访问派生类的成员</strong>。</p><p>多态是面向对象编程的主要特征之一，C++中虚函数的唯一用处就是构成多态,在于具有复杂继承关系的大中型程序，多态可以增加其灵活性，让代码更具有表现力</p><ol><li><strong>纯虚函数</strong></li></ol><p>Line 类表示“线”，没有面积和体积，但它仍然定义了 area() 和 volume() 两个纯虚函数。这样的用意很明显：Line 类不需要被实例化，但是它为派生类提供了“约束条件”，派生类必须要实现这两个函数，完成计算面积和体积的功能，否则就不能实例化</p><h2 id="六：运算符重载"><a href="#六：运算符重载" class="headerlink" title="六：运算符重载"></a>六：运算符重载</h2><ol><li><strong>运算符重载实质</strong></li></ol><p>运算符重载其实就是定义一个函数，在函数体内实现想要的功能，当用到该运算符时，编译器会自动调用这个函数。也就是说，运算符重载是通过函数实现的，它本质上是函数重载。</p><ol><li><p><strong>友元函数</strong></p><p>运算符重载函数不是 complex 类的成员函数，但是却用到了 complex 类的 private 成员变量，所以必须在 complex 类中将该函数声明为友元函数。<strong>即将运算符重载函数作为全局函数时</strong>，二元操作符就需要两个参数，一元操作符需要一个参数，<strong>而且其中必须有一个参数是对象，</strong>好让编译器区分这是程序员自定义的运算符，防止程序员修改用于内置类型的运算符的性质</p></li><li><p><strong>运算符重载的意义</strong></p><p>虽然运算符重载所实现的功能完全可以用函数替代，但运算符重载使得程序的书写更加人性化，易于阅读。运算符被重载后，原有的功能仍然保留，没有丧失或改变。通过运算符重载，扩大了C++已有运算符的功能，使之能用于对象。</p></li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;摘录自&quot;&gt;&lt;a href=&quot;#摘录自&quot; class=&quot;headerlink&quot; title=&quot;摘录自&quot;&gt;&lt;/a&gt;摘录自&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;http://c.biancheng.net/cplus/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://c.biancheng.net/cplus/&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;一&quot;&gt;&lt;a href=&quot;#一&quot; class=&quot;headerlink&quot; title=&quot;一&quot;&gt;&lt;/a&gt;一&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;类只是一张图纸，起到说明的作用，不占用内存空间；对象才是具体的零件，要有地方来存放，才会占用内存空间。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;类只是一个模板（Template），编译后不占用内存空间，所以在定义类时不能对成员变量进行初始化，因为没有地方存储数据。只有在创建对象以后才会给成员变量分配内存，这个时候就可以赋值了&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="C++" scheme="https://LiuCanWu.github.io/tags/C/"/>
    
  </entry>
  
</feed>
